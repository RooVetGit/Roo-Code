#!/usr/bin/env tsx

/**
 * íŒŒì´ì–´ì›” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
 *
 * Docker Compose í™˜ê²½ì—ì„œ ON_PREM ëª¨ë“œì˜ ë„¤íŠ¸ì›Œí¬ ì œí•œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
 * - ë‚´ë¶€ vLLM/Ollama ì„œë²„ ì ‘ê·¼ í—ˆìš© í™•ì¸
 * - ì™¸ë¶€ API í˜¸ì¶œ ì°¨ë‹¨ í™•ì¸
 * - í…”ë ˆë©”íŠ¸ë¦¬ ì°¨ë‹¨ í™•ì¸
 */

import fetch from "node-fetch"
import { createFetchWrapper } from "../../src/utils/fetch-wrapper"
import { OnPremTelemetryClient } from "../../packages/telemetry/src/OnPremTelemetryClient"
import { TelemetryEventName } from "@roo-code/types"

interface TestResult {
	name: string
	passed: boolean
	error?: string
	duration: number
}

class FirewallTester {
	private results: TestResult[] = []
	private vllmUrl: string
	private ollamaUrl: string

	constructor() {
		this.vllmUrl = process.env.VLLM_URL || "http://vllm-server:8000"
		this.ollamaUrl = process.env.OLLAMA_URL || "http://ollama-server:11434"
	}

	async runTest(name: string, testFn: () => Promise<void>): Promise<void> {
		const startTime = Date.now()

		try {
			await testFn()
			this.results.push({
				name,
				passed: true,
				duration: Date.now() - startTime,
			})
			console.log(`âœ… ${name}`)
		} catch (error) {
			this.results.push({
				name,
				passed: false,
				error: error instanceof Error ? error.message : String(error),
				duration: Date.now() - startTime,
			})
			console.log(`âŒ ${name}: ${error}`)
		}
	}

	async testInternalLLMAccess(): Promise<void> {
		await this.runTest("Internal vLLM Server Access", async () => {
			const response = await fetch(`${this.vllmUrl}/v1/models`)
			if (!response.ok) {
				throw new Error(`vLLM server responded with ${response.status}`)
			}
			const data = await response.json()
			if (!data || !Array.isArray(data.data)) {
				throw new Error("Invalid response format from vLLM")
			}
		})

		await this.runTest("Internal Ollama Server Access", async () => {
			const response = await fetch(`${this.ollamaUrl}/api/tags`)
			if (!response.ok) {
				throw new Error(`Ollama server responded with ${response.status}`)
			}
			await response.json()
		})
	}

	async testExternalAPIBlocking(): Promise<void> {
		const wrappedFetch = createFetchWrapper(fetch as any)

		const externalAPIs = [
			"https://api.openai.com/v1/models",
			"https://api.anthropic.com/v1/messages",
			"https://openrouter.ai/api/v1/models",
			"https://us.i.posthog.com/capture",
			"https://api.github.com/repos/test/test",
		]

		for (const url of externalAPIs) {
			await this.runTest(`Block External API: ${url}`, async () => {
				try {
					await wrappedFetch(url)
					throw new Error("External API call should have been blocked")
				} catch (error) {
					if (error instanceof Error && error.message.includes("ON_PREM mode")) {
						// ì •ìƒì ìœ¼ë¡œ ì°¨ë‹¨ë¨
						return
					}
					throw error
				}
			})
		}
	}

	async testTelemetryBlocking(): Promise<void> {
		await this.runTest("Telemetry Blocking", async () => {
			const telemetryClient = new OnPremTelemetryClient(false)

			if (telemetryClient.isTelemetryEnabled()) {
				throw new Error("Telemetry should be disabled in ON_PREM mode")
			}

			// í…”ë ˆë©”íŠ¸ë¦¬ í˜¸ì¶œì´ ë¬´ì‹œë˜ëŠ”ì§€ í™•ì¸
			const result = await telemetryClient.capture({
				event: TelemetryEventName.TASK_CREATED,
				properties: { test: "firewall" },
			})

			if (result !== undefined) {
				throw new Error("Telemetry capture should return undefined")
			}
		})
	}

	async testVLLMChatCompletion(): Promise<void> {
		await this.runTest("vLLM Chat Completion", async () => {
			const response = await fetch(`${this.vllmUrl}/v1/chat/completions`, {
				method: "POST",
				headers: {
					"Content-Type": "application/json",
				},
				body: JSON.stringify({
					model: "microsoft/DialoGPT-small",
					messages: [{ role: "user", content: "Hello, how are you?" }],
					max_tokens: 50,
					temperature: 0.7,
				}),
			})

			if (!response.ok) {
				throw new Error(`vLLM chat completion failed with ${response.status}`)
			}

			const data = await response.json()
			if (!data.choices || !Array.isArray(data.choices) || data.choices.length === 0) {
				throw new Error("Invalid chat completion response")
			}

			if (!data.choices[0].message || !data.choices[0].message.content) {
				throw new Error("No content in chat completion response")
			}
		})
	}

	async testOllamaGeneration(): Promise<void> {
		await this.runTest("Ollama Text Generation", async () => {
			// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
			const modelsResponse = await fetch(`${this.ollamaUrl}/api/tags`)
			if (!modelsResponse.ok) {
				throw new Error("Failed to fetch Ollama models")
			}

			const models = await modelsResponse.json()
			if (!models.models || models.models.length === 0) {
				// ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
				console.log("âš ï¸ No Ollama models available, skipping generation test")
				return
			}

			const modelName = models.models[0].name

			const response = await fetch(`${this.ollamaUrl}/api/generate`, {
				method: "POST",
				headers: {
					"Content-Type": "application/json",
				},
				body: JSON.stringify({
					model: modelName,
					prompt: "Hello, how are you?",
					stream: false,
				}),
			})

			if (!response.ok) {
				throw new Error(`Ollama generation failed with ${response.status}`)
			}

			const data = await response.json()
			if (!data.response) {
				throw new Error("No response from Ollama generation")
			}
		})
	}

	async testNetworkIsolation(): Promise<void> {
		await this.runTest("Network Isolation Check", async () => {
			// íŒŒì´ì–´ì›” ìƒíƒœ í™•ì¸
			const firewallResponse = await fetch("http://firewall-proxy/firewall/status")
			if (!firewallResponse.ok) {
				throw new Error("Firewall proxy not responding")
			}

			const firewallStatus = await firewallResponse.json()
			if (firewallStatus.status !== "active" || firewallStatus.mode !== "ON_PREM") {
				throw new Error("Firewall not in correct mode")
			}
		})

		await this.runTest("External Domain Resolution Block", async () => {
			try {
				// DNS í•´ê²°ì€ ë˜ì§€ë§Œ HTTP ì—°ê²°ì´ ì°¨ë‹¨ë˜ì–´ì•¼ í•¨
				const response = await fetch("http://api.openai.com", {
					timeout: 5000,
				})

				// ì—°ê²°ì´ ì„±ê³µí•˜ë©´ ì•ˆë¨
				if (response.ok) {
					throw new Error("External domain should be blocked")
				}
			} catch (error) {
				// ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë‚˜ íƒ€ì„ì•„ì›ƒì€ ì •ìƒ (ì°¨ë‹¨ë¨)
				if (error instanceof Error) {
					if (
						error.message.includes("timeout") ||
						error.message.includes("ENOTFOUND") ||
						error.message.includes("ECONNREFUSED")
					) {
						return // ì •ìƒì ìœ¼ë¡œ ì°¨ë‹¨ë¨
					}
				}
				throw error
			}
		})
	}

	async testPerformance(): Promise<void> {
		await this.runTest("vLLM Response Time", async () => {
			const startTime = Date.now()

			const response = await fetch(`${this.vllmUrl}/v1/models`)
			if (!response.ok) {
				throw new Error("vLLM models endpoint failed")
			}

			const duration = Date.now() - startTime
			if (duration > 5000) {
				throw new Error(`vLLM response too slow: ${duration}ms`)
			}

			console.log(`   vLLM response time: ${duration}ms`)
		})

		await this.runTest("Ollama Response Time", async () => {
			const startTime = Date.now()

			const response = await fetch(`${this.ollamaUrl}/api/tags`)
			if (!response.ok) {
				throw new Error("Ollama tags endpoint failed")
			}

			const duration = Date.now() - startTime
			if (duration > 5000) {
				throw new Error(`Ollama response too slow: ${duration}ms`)
			}

			console.log(`   Ollama response time: ${duration}ms`)
		})
	}

	async runAllTests(): Promise<void> {
		console.log("ğŸ”¥ Starting Firewall Tests...")
		console.log(`ğŸ“¡ vLLM URL: ${this.vllmUrl}`)
		console.log(`ğŸ¦™ Ollama URL: ${this.ollamaUrl}`)
		console.log(`ğŸ”’ ON_PREM Mode: ${process.env.ON_PREM}`)
		console.log("")

		// ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
		await this.testNetworkIsolation()

		// ë‚´ë¶€ ì„œë¹„ìŠ¤ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
		await this.testInternalLLMAccess()

		// ì™¸ë¶€ API ì°¨ë‹¨ í…ŒìŠ¤íŠ¸
		await this.testExternalAPIBlocking()

		// í…”ë ˆë©”íŠ¸ë¦¬ ì°¨ë‹¨ í…ŒìŠ¤íŠ¸
		await this.testTelemetryBlocking()

		// LLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
		await this.testVLLMChatCompletion()
		await this.testOllamaGeneration()

		// ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
		await this.testPerformance()

		this.printResults()
	}

	private printResults(): void {
		console.log("\n" + "=".repeat(60))
		console.log("ğŸ§ª FIREWALL TEST RESULTS")
		console.log("=".repeat(60))

		const passed = this.results.filter((r) => r.passed).length
		const failed = this.results.filter((r) => !r.passed).length
		const total = this.results.length

		console.log(`ğŸ“Š Total: ${total}, Passed: ${passed}, Failed: ${failed}`)
		console.log("")

		if (failed > 0) {
			console.log("âŒ FAILED TESTS:")
			this.results
				.filter((r) => !r.passed)
				.forEach((r) => {
					console.log(`   â€¢ ${r.name}: ${r.error}`)
				})
			console.log("")
		}

		console.log("â±ï¸  PERFORMANCE:")
		this.results.forEach((r) => {
			const status = r.passed ? "âœ…" : "âŒ"
			console.log(`   ${status} ${r.name}: ${r.duration}ms`)
		})

		console.log("\n" + "=".repeat(60))

		if (failed === 0) {
			console.log("ğŸ‰ ALL TESTS PASSED! ON_PREM mode is working correctly.")
		} else {
			console.log(`ğŸ’¥ ${failed} TEST(S) FAILED. Check the errors above.`)
			process.exit(1)
		}
	}
}

// ë©”ì¸ ì‹¤í–‰
async function main() {
	// ON_PREM ëª¨ë“œ ê°•ì œ í™œì„±í™”
	process.env.ON_PREM = "true"

	const tester = new FirewallTester()
	await tester.runAllTests()
}

if (require.main === module) {
	main().catch((error) => {
		console.error("ğŸ’¥ Test execution failed:", error)
		process.exit(1)
	})
}
