import { test, expect } from "@playwright/test"
import { spawn, type ChildProcess } from "child_process"
import { promisify } from "util"
import fetch from "node-fetch"

/**
 * ON_PREM + vLLM E2E í…ŒìŠ¤íŠ¸
 *
 * ì‹¤ì œ vLLM ì„œë²„ì™€ VS Code í™•ì¥ì˜ í†µí•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
 * Dockerë¡œ vLLM ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  í™•ì¥ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.
 */

interface VLLMServerConfig {
	host: string
	port: number
	model: string
}

class VLLMTestServer {
	private process: ChildProcess | null = null
	private config: VLLMServerConfig

	constructor(config: VLLMServerConfig) {
		this.config = config
	}

	async start(): Promise<void> {
		console.log("ğŸš€ Starting vLLM test server...")

		// vLLM ì„œë²„ë¥¼ Dockerë¡œ ì‹œì‘
		this.process = spawn(
			"docker",
			[
				"run",
				"--rm",
				"-p",
				`${this.config.port}:8000`,
				"--gpus",
				"all", // GPU ì‚¬ìš© (ì—†ìœ¼ë©´ CPU ëª¨ë“œë¡œ ëŒ€ì²´)
				"vllm/vllm-openai:latest",
				"--model",
				this.config.model,
				"--host",
				"0.0.0.0",
				"--port",
				"8000",
			],
			{
				stdio: ["ignore", "pipe", "pipe"],
			},
		)

		if (this.process.stdout) {
			this.process.stdout.on("data", (data) => {
				console.log(`[vLLM] ${data}`)
			})
		}

		if (this.process.stderr) {
			this.process.stderr.on("data", (data) => {
				console.error(`[vLLM Error] ${data}`)
			})
		}

		// ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 120ì´ˆ)
		await this.waitForReady(120000)
	}

	async stop(): Promise<void> {
		if (this.process) {
			console.log("ğŸ›‘ Stopping vLLM test server...")
			this.process.kill("SIGTERM")

			// í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
			await new Promise((resolve) => {
				this.process!.on("exit", resolve)
				setTimeout(resolve, 5000) // 5ì´ˆ í›„ ê°•ì œ ì¢…ë£Œ
			})

			this.process = null
		}
	}

	private async waitForReady(timeoutMs: number): Promise<void> {
		const startTime = Date.now()
		const url = `http://${this.config.host}:${this.config.port}/v1/models`

		while (Date.now() - startTime < timeoutMs) {
			try {
				const response = await fetch(url)
				if (response.ok) {
					console.log("âœ… vLLM server is ready!")
					return
				}
			} catch (error) {
				// ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ
			}

			await new Promise((resolve) => setTimeout(resolve, 2000))
		}

		throw new Error(`vLLM server failed to start within ${timeoutMs}ms`)
	}

	getBaseUrl(): string {
		return `http://${this.config.host}:${this.config.port}`
	}
}

test.describe("ON_PREM vLLM E2E Tests", () => {
	let vllmServer: VLLMTestServer
	const serverConfig: VLLMServerConfig = {
		host: "localhost",
		port: 8000,
		model: "microsoft/DialoGPT-small", // ì‘ì€ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
	}

	test.beforeAll(async () => {
		// CI í™˜ê²½ì—ì„œëŠ” ìŠ¤í‚µ (ì‹¤ì œ GPUê°€ í•„ìš”í•¨)
		if (process.env.CI) {
			test.skip()
		}

		vllmServer = new VLLMTestServer(serverConfig)

		try {
			await vllmServer.start()
		} catch (error) {
			console.warn("âš ï¸ vLLM server ì‹œì‘ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
			test.skip()
		}
	})

	test.afterAll(async () => {
		if (vllmServer) {
			await vllmServer.stop()
		}
	})

	test("should connect to vLLM server in ON_PREM mode", async ({ page }) => {
		// VS Code í™•ì¥ í™˜ê²½ ì„¤ì •
		await page.addInitScript(() => {
			// ON_PREM ëª¨ë“œ í™œì„±í™”
			window.process = {
				env: { ON_PREM: "true" },
			} as any

			// localLLM ì„¤ì • ëª¨í‚¹
			window.vscode = {
				workspace: {
					getConfiguration: (section: string) => {
						if (section === "roo-cline") {
							return {
								get: (key: string) => {
									if (key === "localLLM") {
										return {
											type: "vllm",
											url: "http://localhost:8000",
											model: "microsoft/DialoGPT-small",
										}
									}
									return undefined
								},
							}
						}
						return { get: () => undefined }
					},
				},
			} as any
		})

		// í™•ì¥ í˜ì´ì§€ ë¡œë“œ
		await page.goto("vscode://roo-code.roo-cline")

		// vLLM ì—°ê²° í™•ì¸ ë²„íŠ¼ í´ë¦­ (UIì— ì´ëŸ° ê¸°ëŠ¥ì´ ìˆë‹¤ê³  ê°€ì •)
		await page.click("[data-testid='test-llm-connection']")

		// ì—°ê²° ì„±ê³µ ë©”ì‹œì§€ í™•ì¸
		await expect(page.locator("[data-testid='connection-status']")).toContainText("Connected to vLLM")

		// ì™¸ë¶€ API í˜¸ì¶œ ì°¨ë‹¨ í™•ì¸
		const externalCallResult = await page.evaluate(async () => {
			try {
				await fetch("https://api.openai.com/v1/models")
				return "success"
			} catch (error) {
				return error.message
			}
		})

		expect(externalCallResult).toContain("ON_PREM mode: External HTTP calls are disabled")
	})

	test("should generate text using vLLM in ON_PREM mode", async ({ page }) => {
		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
			window.vscode = {
				workspace: {
					getConfiguration: () => ({
						get: (key: string) => {
							if (key === "localLLM") {
								return {
									type: "vllm",
									url: "http://localhost:8000",
									model: "microsoft/DialoGPT-small",
								}
							}
							return undefined
						},
					}),
				},
			} as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ìƒˆ ì±„íŒ… ì‹œì‘
		await page.click("[data-testid='new-chat']")

		// ê°„ë‹¨í•œ ì§ˆë¬¸ ì…ë ¥
		await page.fill("[data-testid='chat-input']", "Hello, how are you?")
		await page.click("[data-testid='send-message']")

		// vLLM ì‘ë‹µ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
		await page.waitForSelector("[data-testid='ai-response']", { timeout: 30000 })

		// ì‘ë‹µì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
		const response = await page.textContent("[data-testid='ai-response']")
		expect(response).toBeTruthy()
		expect(response!.length).toBeGreaterThan(0)

		// ì‘ë‹µì— vLLM ì œê³µì ì •ë³´ê°€ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸
		await expect(page.locator("[data-testid='provider-info']")).toContainText("vLLM")
	})

	test("should handle vLLM streaming responses", async ({ page }) => {
		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸
		await page.fill("[data-testid='chat-input']", "Write a short story about a robot.")
		await page.click("[data-testid='send-message']")

		// ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ í™•ì¸
		await page.waitForSelector("[data-testid='streaming-indicator']", { timeout: 5000 })

		// í…ìŠ¤íŠ¸ê°€ ì ì§„ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
		let previousLength = 0
		let streamingWorking = false

		for (let i = 0; i < 10; i++) {
			await page.waitForTimeout(1000)
			const currentText = (await page.textContent("[data-testid='ai-response']")) || ""

			if (currentText.length > previousLength) {
				streamingWorking = true
				previousLength = currentText.length
			}
		}

		expect(streamingWorking).toBe(true)

		// ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í™•ì¸
		await page.waitForSelector("[data-testid='streaming-indicator']", {
			state: "hidden",
			timeout: 30000,
		})
	})

	test("should fallback gracefully when vLLM is unavailable", async ({ page }) => {
		// ì˜ëª»ëœ vLLM URL ì„¤ì •
		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
			window.vscode = {
				workspace: {
					getConfiguration: () => ({
						get: (key: string) => {
							if (key === "localLLM") {
								return {
									type: "vllm",
									url: "http://localhost:9999", // ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í¬íŠ¸
									model: "invalid-model",
								}
							}
							return undefined
						},
					}),
				},
			} as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ì—°ê²° í…ŒìŠ¤íŠ¸
		await page.click("[data-testid='test-llm-connection']")

		// ì—°ê²° ì‹¤íŒ¨ ë©”ì‹œì§€ í™•ì¸
		await expect(page.locator("[data-testid='connection-error']")).toContainText("Failed to connect")

		// ì±„íŒ… ì‹œë„
		await page.fill("[data-testid='chat-input']", "Hello")
		await page.click("[data-testid='send-message']")

		// ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
		await expect(page.locator("[data-testid='error-message']")).toContainText("vLLM server is not available")
	})

	test("should respect ON_PREM telemetry settings", async ({ page }) => {
		// í…”ë ˆë©”íŠ¸ë¦¬ í˜¸ì¶œ ëª¨ë‹ˆí„°ë§
		const telemetryRequests: string[] = []

		await page.route("**/capture", (route, request) => {
			telemetryRequests.push(request.url())
			route.abort()
		})

		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ì—¬ëŸ¬ ì•¡ì…˜ ìˆ˜í–‰
		await page.click("[data-testid='new-chat']")
		await page.fill("[data-testid='chat-input']", "Test message")
		await page.click("[data-testid='send-message']")

		// ì ì‹œ ëŒ€ê¸°
		await page.waitForTimeout(2000)

		// í…”ë ˆë©”íŠ¸ë¦¬ í˜¸ì¶œì´ ì°¨ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
		expect(telemetryRequests).toHaveLength(0)
	})

	test("should validate vLLM model configuration", async ({ page }) => {
		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ì„¤ì • í˜ì´ì§€ë¡œ ì´ë™
		await page.click("[data-testid='settings-button']")

		// vLLM ì„¤ì • ì„¹ì…˜ í™•ì¸
		await expect(page.locator("[data-testid='vllm-settings']")).toBeVisible()

		// í˜„ì¬ ëª¨ë¸ ì •ë³´ í‘œì‹œ í™•ì¸
		await expect(page.locator("[data-testid='current-model']")).toContainText("microsoft/DialoGPT-small")

		// URL ìœ íš¨ì„± ê²€ì‚¬
		await page.fill("[data-testid='vllm-url']", "invalid-url")
		await page.click("[data-testid='validate-config']")
		await expect(page.locator("[data-testid='validation-error']")).toContainText("Invalid URL format")

		// ì˜¬ë°”ë¥¸ URL ì„¤ì •
		await page.fill("[data-testid='vllm-url']", "http://localhost:8000")
		await page.click("[data-testid='validate-config']")
		await expect(page.locator("[data-testid='validation-success']")).toContainText("Configuration valid")
	})
})

test.describe("ON_PREM Ollama E2E Tests", () => {
	test("should work with Ollama in ON_PREM mode", async ({ page }) => {
		// Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
		try {
			const response = await fetch("http://localhost:11434/api/tags")
			if (!response.ok) {
				test.skip()
			}
		} catch {
			test.skip() // Ollama ì„œë²„ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
		}

		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
			window.vscode = {
				workspace: {
					getConfiguration: () => ({
						get: (key: string) => {
							if (key === "localLLM") {
								return {
									type: "ollama",
									url: "http://localhost:11434",
									model: "llama2:7b",
								}
							}
							return undefined
						},
					}),
				},
			} as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
		await page.click("[data-testid='test-llm-connection']")
		await expect(page.locator("[data-testid='connection-status']")).toContainText("Connected to Ollama")

		// í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
		await page.fill("[data-testid='chat-input']", "What is AI?")
		await page.click("[data-testid='send-message']")

		await page.waitForSelector("[data-testid='ai-response']", { timeout: 30000 })
		const response = await page.textContent("[data-testid='ai-response']")
		expect(response).toBeTruthy()
	})
})

test.describe("Firewall Simulation Tests", () => {
	test("should handle network restrictions properly", async ({ page }) => {
		// ëª¨ë“  ì™¸ë¶€ ìš”ì²­ ì°¨ë‹¨
		await page.route("**/*", (route, request) => {
			const url = new URL(request.url())

			// ë¡œì»¬/ë‚´ë¶€ í˜¸ì¶œë§Œ í—ˆìš©
			if (url.hostname === "localhost" || url.hostname === "127.0.0.1" || !url.hostname.includes(".")) {
				route.continue()
			} else {
				route.abort("failed")
			}
		})

		await page.addInitScript(() => {
			window.process = { env: { ON_PREM: "true" } } as any
		})

		await page.goto("vscode://roo-code.roo-cline")

		// ì™¸ë¶€ API í˜¸ì¶œ ì‹œë„ (ì°¨ë‹¨ë˜ì–´ì•¼ í•¨)
		const blockedResult = await page.evaluate(async () => {
			try {
				await fetch("https://api.openai.com/v1/models")
				return "allowed"
			} catch (error) {
				return "blocked"
			}
		})

		expect(blockedResult).toBe("blocked")

		// ë‚´ë¶€ í˜¸ì¶œì€ í—ˆìš©ë˜ì–´ì•¼ í•¨
		const allowedResult = await page.evaluate(async () => {
			try {
				await fetch("http://localhost:8000/v1/models")
				return "allowed"
			} catch (error) {
				return "blocked"
			}
		})

		expect(allowedResult).toBe("allowed")
	})
})
