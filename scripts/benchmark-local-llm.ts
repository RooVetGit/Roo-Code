#!/usr/bin/env tsx

/**
 * Local LLM ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
 * vLLM vs Ollama ì„±ëŠ¥ ë¹„êµ
 *
 * ì‚¬ìš©ë²•:
 * npm run benchmark-local-llm
 *
 * í™˜ê²½ë³€ìˆ˜:
 * VLLM_URL=http://gpu-srv:8000 npm run benchmark-local-llm
 * OLLAMA_URL=http://localhost:11434 npm run benchmark-local-llm
 */

import { performance } from "perf_hooks"

interface BenchmarkConfig {
	vllmUrl: string
	ollamaUrl: string
	model: string
	testPrompts: string[]
	iterations: number
}

interface BenchmarkResult {
	provider: "vLLM" | "Ollama"
	url: string
	model: string
	avgResponseTime: number
	minResponseTime: number
	maxResponseTime: number
	tokensPerSecond: number
	successRate: number
	errors: string[]
}

interface TestResult {
	success: boolean
	responseTime: number
	tokenCount: number
	error?: string
}

/**
 * HTTP ìš”ì²­ ë²¤ì¹˜ë§ˆí¬
 */
async function benchmarkProvider(
	providerName: "vLLM" | "Ollama",
	baseUrl: string,
	model: string,
	prompts: string[],
	iterations: number,
): Promise<BenchmarkResult> {
	const results: TestResult[] = []
	const errors: string[] = []

	console.log(`\nğŸš€ ${providerName} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (${baseUrl})`)
	console.log(`ëª¨ë¸: ${model}, ë°˜ë³µ: ${iterations}íšŒ`)

	for (let i = 0; i < iterations; i++) {
		const prompt = prompts[i % prompts.length]

		try {
			const result = await testSingleRequest(providerName, baseUrl, model, prompt)
			results.push(result)

			if (result.success) {
				process.stdout.write("âœ… ")
			} else {
				process.stdout.write("âŒ ")
				if (result.error) {
					errors.push(result.error)
				}
			}
		} catch (error) {
			const errorMsg = error instanceof Error ? error.message : String(error)
			errors.push(errorMsg)
			results.push({
				success: false,
				responseTime: 0,
				tokenCount: 0,
				error: errorMsg,
			})
			process.stdout.write("ğŸ’¥ ")
		}

		// ìš”ì²­ ê°„ ê°„ê²©
		if (i < iterations - 1) {
			await new Promise((resolve) => setTimeout(resolve, 100))
		}
	}

	console.log() // ì¤„ë°”ê¿ˆ

	// ì„±ê³µí•œ ê²°ê³¼ë§Œ ê³„ì‚°
	const successResults = results.filter((r) => r.success)
	const responseTimes = successResults.map((r) => r.responseTime)
	const tokenCounts = successResults.map((r) => r.tokenCount)

	if (successResults.length === 0) {
		return {
			provider: providerName,
			url: baseUrl,
			model,
			avgResponseTime: 0,
			minResponseTime: 0,
			maxResponseTime: 0,
			tokensPerSecond: 0,
			successRate: 0,
			errors,
		}
	}

	const avgResponseTime = responseTimes.reduce((a, b) => a + b, 0) / responseTimes.length
	const totalTokens = tokenCounts.reduce((a, b) => a + b, 0)
	const totalTime = responseTimes.reduce((a, b) => a + b, 0) / 1000 // ì´ˆ ë‹¨ìœ„

	return {
		provider: providerName,
		url: baseUrl,
		model,
		avgResponseTime,
		minResponseTime: Math.min(...responseTimes),
		maxResponseTime: Math.max(...responseTimes),
		tokensPerSecond: totalTime > 0 ? totalTokens / totalTime : 0,
		successRate: (successResults.length / results.length) * 100,
		errors,
	}
}

/**
 * ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸
 */
async function testSingleRequest(
	provider: "vLLM" | "Ollama",
	baseUrl: string,
	model: string,
	prompt: string,
): Promise<TestResult> {
	const startTime = performance.now()

	try {
		let apiUrl: string
		let requestBody: any

		if (provider === "vLLM") {
			// vLLM OpenAI í˜¸í™˜ API
			apiUrl = baseUrl.endsWith("/v1") ? `${baseUrl}/chat/completions` : `${baseUrl}/v1/chat/completions`

			requestBody = {
				model,
				messages: [{ role: "user", content: prompt }],
				max_tokens: 100,
				temperature: 0.7,
			}
		} else {
			// Ollama API
			apiUrl = baseUrl.endsWith("/api") ? `${baseUrl}/generate` : `${baseUrl}/api/generate`

			requestBody = {
				model,
				prompt,
				stream: false,
				options: {
					num_predict: 100,
					temperature: 0.7,
				},
			}
		}

		const response = await fetch(apiUrl, {
			method: "POST",
			headers: {
				"Content-Type": "application/json",
			},
			body: JSON.stringify(requestBody),
			signal: AbortSignal.timeout(30000), // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
		})

		const endTime = performance.now()
		const responseTime = endTime - startTime

		if (!response.ok) {
			return {
				success: false,
				responseTime,
				tokenCount: 0,
				error: `HTTP ${response.status}: ${response.statusText}`,
			}
		}

		const data = await response.json()
		let tokenCount = 0

		if (provider === "vLLM") {
			// OpenAI í˜•ì‹ ì‘ë‹µ
			tokenCount = data.usage?.completion_tokens || 0
		} else {
			// Ollama ì‘ë‹µì—ì„œ í† í° ì¶”ì • (ê³µë°± ê¸°ì¤€)
			const responseText = data.response || ""
			tokenCount = responseText.split(/\s+/).length
		}

		return {
			success: true,
			responseTime,
			tokenCount,
		}
	} catch (error) {
		const endTime = performance.now()
		return {
			success: false,
			responseTime: endTime - startTime,
			tokenCount: 0,
			error: error instanceof Error ? error.message : String(error),
		}
	}
}

/**
 * ê²°ê³¼ ì¶œë ¥
 */
function printResults(results: BenchmarkResult[]) {
	console.log("\n" + "=".repeat(80))
	console.log("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
	console.log("=".repeat(80))

	results.forEach((result) => {
		console.log(`\nğŸ¤– ${result.provider} (${result.url})`)
		console.log(`ëª¨ë¸: ${result.model}`)
		console.log(`ì„±ê³µë¥ : ${result.successRate.toFixed(1)}%`)

		if (result.successRate > 0) {
			console.log(`í‰ê·  ì‘ë‹µì‹œê°„: ${result.avgResponseTime.toFixed(2)}ms`)
			console.log(`ìµœì†Œ ì‘ë‹µì‹œê°„: ${result.minResponseTime.toFixed(2)}ms`)
			console.log(`ìµœëŒ€ ì‘ë‹µì‹œê°„: ${result.maxResponseTime.toFixed(2)}ms`)
			console.log(`í† í°/ì´ˆ: ${result.tokensPerSecond.toFixed(2)}`)
		}

		if (result.errors.length > 0) {
			console.log(`ì˜¤ë¥˜ (ìµœëŒ€ 3ê°œ): ${result.errors.slice(0, 3).join(", ")}`)
		}
	})

	// ë¹„êµ ê²°ê³¼
	if (results.length === 2) {
		const [first, second] = results
		console.log(`\nğŸ† ë¹„êµ ê²°ê³¼`)

		if (first.successRate > second.successRate) {
			console.log(
				`ì„±ê³µë¥ : ${first.provider} ìŠ¹ (${first.successRate.toFixed(1)}% vs ${second.successRate.toFixed(1)}%)`,
			)
		} else if (second.successRate > first.successRate) {
			console.log(
				`ì„±ê³µë¥ : ${second.provider} ìŠ¹ (${second.successRate.toFixed(1)}% vs ${first.successRate.toFixed(1)}%)`,
			)
		} else {
			console.log(`ì„±ê³µë¥ : ë™ì  (${first.successRate.toFixed(1)}%)`)
		}

		if (first.avgResponseTime < second.avgResponseTime) {
			console.log(
				`ì‘ë‹µì†ë„: ${first.provider} ìŠ¹ (${first.avgResponseTime.toFixed(2)}ms vs ${second.avgResponseTime.toFixed(2)}ms)`,
			)
		} else if (second.avgResponseTime < first.avgResponseTime) {
			console.log(
				`ì‘ë‹µì†ë„: ${second.provider} ìŠ¹ (${second.avgResponseTime.toFixed(2)}ms vs ${first.avgResponseTime.toFixed(2)}ms)`,
			)
		}

		if (first.tokensPerSecond > second.tokensPerSecond) {
			console.log(
				`ì²˜ë¦¬ëŸ‰: ${first.provider} ìŠ¹ (${first.tokensPerSecond.toFixed(2)} vs ${second.tokensPerSecond.toFixed(2)} í† í°/ì´ˆ)`,
			)
		} else if (second.tokensPerSecond > first.tokensPerSecond) {
			console.log(
				`ì²˜ë¦¬ëŸ‰: ${second.provider} ìŠ¹ (${second.tokensPerSecond.toFixed(2)} vs ${first.tokensPerSecond.toFixed(2)} í† í°/ì´ˆ)`,
			)
		}
	}
}

/**
 * ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
 */
async function main() {
	const config: BenchmarkConfig = {
		vllmUrl: process.env.VLLM_URL || "http://localhost:8000",
		ollamaUrl: process.env.OLLAMA_URL || "http://localhost:11434",
		model: process.env.MODEL || "llama2",
		iterations: parseInt(process.env.ITERATIONS || "10"),
		testPrompts: [
			"Hello, how are you?",
			"Explain quantum computing in simple terms.",
			"Write a short Python function to calculate fibonacci numbers.",
			"What are the benefits of renewable energy?",
			"Describe the water cycle process.",
		],
	}

	console.log("ğŸ¯ Local LLM ë²¤ì¹˜ë§ˆí¬ ë„êµ¬")
	console.log(`vLLM: ${config.vllmUrl}`)
	console.log(`Ollama: ${config.ollamaUrl}`)
	console.log(`ëª¨ë¸: ${config.model}`)
	console.log(`ë°˜ë³µ: ${config.iterations}íšŒ`)

	const results: BenchmarkResult[] = []

	// vLLM ë²¤ì¹˜ë§ˆí¬
	try {
		const vllmResult = await benchmarkProvider(
			"vLLM",
			config.vllmUrl,
			config.model,
			config.testPrompts,
			config.iterations,
		)
		results.push(vllmResult)
	} catch (error) {
		console.error(`âŒ vLLM ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: ${error}`)
	}

	// Ollama ë²¤ì¹˜ë§ˆí¬
	try {
		const ollamaResult = await benchmarkProvider(
			"Ollama",
			config.ollamaUrl,
			config.model,
			config.testPrompts,
			config.iterations,
		)
		results.push(ollamaResult)
	} catch (error) {
		console.error(`âŒ Ollama ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: ${error}`)
	}

	// ê²°ê³¼ ì¶œë ¥
	printResults(results)
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if (require.main === module) {
	main().catch((error) => {
		console.error("ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜¤ë¥˜:", error)
		process.exit(1)
	})
}
